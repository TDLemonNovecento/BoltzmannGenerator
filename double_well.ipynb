{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import FrEIA.framework as Ff\n",
    "import FrEIA.modules as Fm\n",
    "from matplotlib import pyplot as pp\n",
    "import math\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\") if use_cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleWell():\n",
    "    \"\"\"A simple double well potential in 2-D.\"\"\"\n",
    "    def __init__(self, a1=1.0, a2=6.0, a4=1.0, k=1.0):\n",
    "        self.a1 = a1\n",
    "        self.a2 = a2\n",
    "        self.a4 = a4\n",
    "        self.k = k\n",
    "\n",
    "    def energy(self, x):\n",
    "        dimer_energy = (self.a4 * x[0]**4 - \n",
    "                        self.a2 * x[0]**2 + \n",
    "                        self.a1 * x[0])\n",
    "        \n",
    "        oscillator_energy = 0.5 * self.k * x[1] ** 2\n",
    "        return  dimer_energy + oscillator_energy\n",
    "\n",
    "    def forces(self, x):\n",
    "        dimer_force = (4 * self.a4 * x[0]**3 -\n",
    "                       2 * self.a2 * x[0] +\n",
    "                       self.a1)\n",
    "        oscillator_force = self.k * x[1]\n",
    "        return np.array([dimer_force, oscillator_force])\n",
    "    \n",
    "    def get_dimer_energies(self):\n",
    "        \"\"\" Plots the dimer energy to the standard figure \"\"\"\n",
    "        x_grid = np.linspace(-3, 3, num=200)\n",
    "        energies = []\n",
    "        for x in x_grid:\n",
    "            energies.append(self.energy([x, 0.0]))\n",
    "        return x_grid, np.array(energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid, energies = DoubleWell().get_dimer_energies()\n",
    "pp.plot(x_grid, energies);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metropolis():\n",
    "    def __init__(self, model, x0, noise=0.1, burnin=0, stride=1):\n",
    "        \"\"\"A simple Metropolis Monte Carlo sampler.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model: Energy model\n",
    "            An object that provides the function energy(x) that\n",
    "            computes the energy.\n",
    "        x0: np.array\n",
    "            The initial configuration\n",
    "        noise: float\n",
    "            The size of the gaussian proposal step.\n",
    "        burnin: int\n",
    "            The number of initial steps to discard.\n",
    "        stride: int\n",
    "            How often output is saved.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.noise = noise\n",
    "        self.burnin = burnin\n",
    "        self.stride = stride\n",
    "        self.x = None\n",
    "        self.step = None\n",
    "        self.traj_ = None\n",
    "        self.etraj_ = None\n",
    "        self.energy = None\n",
    "        self.reset(x0)\n",
    "        \n",
    "    def reset(self, x0):\n",
    "        \"\"\"Reset the sampler.\n",
    "        \n",
    "        This will set the sampler to a new point and discard\n",
    "        previous data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: np.ndarray\n",
    "            New value for the sampler.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.step = 0\n",
    "        self.traj_ = []\n",
    "        self.etraj_ = []\n",
    "        self.x = x0\n",
    "        self.energy = self.model.energy(self.x)\n",
    "\n",
    "    def run(self, n_steps):\n",
    "        for _ in range(n_steps):\n",
    "            self.step += 1\n",
    "            self._run_trial()\n",
    "            self._log_results()\n",
    "        \n",
    "    def _log_results(self):\n",
    "        if self.step > self.burnin:\n",
    "            if self.step % self.stride == 0:\n",
    "                self.traj_.append(self.x)\n",
    "                self.etraj_.append(self.energy)\n",
    "    \n",
    "    def _run_trial(self):\n",
    "        x_prop = self.x + self.noise * np.random.randn(self.x.shape[0])\n",
    "        e_prop = self.model.energy(x_prop)\n",
    "        if e_prop <= self.energy:\n",
    "            accept = True\n",
    "        else:\n",
    "            metrop = math.exp(self.energy - e_prop)\n",
    "            if np.random.rand() < metrop:\n",
    "                accept = True\n",
    "            else:\n",
    "                accept = False\n",
    "        if accept:\n",
    "            self.x = x_prop\n",
    "            self.energy = e_prop\n",
    "    \n",
    "    @property\n",
    "    def traj(self):\n",
    "        return np.array(self.traj_)\n",
    "    \n",
    "    @property\n",
    "    def etraj(self):\n",
    "        return np.array(self.etraj_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_left = np.array([-1.7, 0.0])\n",
    "x0_right = np.array([1.7, 0.0])\n",
    "nsteps = 20_000\n",
    "ener_model = DoubleWell()\n",
    "\n",
    "# Generate data starting from left well.\n",
    "sampler = Metropolis(ener_model, x0_left, stride=2)\n",
    "sampler.run(nsteps)\n",
    "traj_left = sampler.traj\n",
    "\n",
    "# Generate data starting from right well.\n",
    "sampler.reset(x0_right)\n",
    "sampler.run(nsteps)\n",
    "traj_right = sampler.traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the x-components.\n",
    "pp.plot(traj_left[:, 0])\n",
    "pp.plot(traj_right[:, 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the trajectory and shuffle into random order\n",
    "combined_traj = np.vstack([traj_left, traj_right])\n",
    "np.random.shuffle(combined_traj)\n",
    "\n",
    "# divide into validation and training sets\n",
    "n_validation = combined_traj.shape[0] // 10\n",
    "traj_valid = combined_traj[:n_validation, :]\n",
    "traj_train = combined_traj[n_validation:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a scatter plot of the combined trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.scatter(combined_traj[:, 0], combined_traj[:, 1], marker=\".\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertible Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrEIA needs a function to create the non-linear layers for the invertible modules. This function takes\n",
    "the number of inputs and outputs as parameters and should construct a network.\n",
    "\n",
    "In this case, we use 3 linear - ReLU layers with 100 hidden units, followed by a final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateFC:\n",
    "    def __init__(self, n_hidden):\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "    def __call__(self, c_in, c_out):\n",
    "        lin1 = nn.Linear(c_in, self.n_hidden)\n",
    "        lin2 = nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        lin3 = nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        lin4 = nn.Linear(self.n_hidden, c_out)\n",
    "        \n",
    "        # Initialize the weights in each layer.\n",
    "        # Kaiming initialization is suitable for\n",
    "        # ReLU activation functions.\n",
    "        torch.nn.init.kaiming_uniform_(lin1.weight)\n",
    "        torch.nn.init.kaiming_uniform_(lin2.weight)\n",
    "        torch.nn.init.kaiming_uniform_(lin3.weight)\n",
    "        # Initialize the weights and biases in the last\n",
    "        # Layer to zero, which gives the identity transform\n",
    "        # as our starting point.\n",
    "        torch.nn.init.zeros_(lin4.weight)\n",
    "        torch.nn.init.zeros_(lin4.bias)\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            lin1,\n",
    "            nn.ReLU(),\n",
    "            lin2,\n",
    "            nn.ReLU(),\n",
    "            lin3,\n",
    "            nn.ReLU(),\n",
    "            lin4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build our invertible network. This is very flexible. We currently use 4 GLOW layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our reversible network\n",
    "\n",
    "nodes = [Ff.InputNode(2, name=\"input\")]\n",
    "\n",
    "n_glow_layers = 8\n",
    "for i in range(n_glow_layers):\n",
    "    node1 = Ff.Node(\n",
    "        nodes[-1],\n",
    "        Fm.GLOWCouplingBlock,\n",
    "        {\"subnet_constructor\": CreateFC(64), \"clamp\": 2},\n",
    "        name=f\"glow_{i}\"\n",
    "    )\n",
    "    node2 = Ff.Node(node1, Fm.PermuteRandom,\n",
    "                   {\"seed\": i},\n",
    "                   name=f\"permute_{i}\")\n",
    "    nodes.append(node1)\n",
    "    nodes.append(node2)\n",
    "    \n",
    "nodes.append(Ff.OutputNode(nodes[-1], name=\"output\"))\n",
    "net = Ff.ReversibleGraphNet(nodes, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the network and training / validation data to the device\n",
    "net = net.to(device=device)\n",
    "traj_train = torch.as_tensor(traj_train, device=device, dtype=torch.float32)\n",
    "traj_valid = torch.as_tensor(traj_valid, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 2000  # This is more than necessary\n",
    "n_batch = 256\n",
    "\n",
    "# This is the number of data points per batch\n",
    "I = np.arange(traj_train.shape[0])  # A list of indices into the training set\n",
    "\n",
    "# Build the optimizer.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True)\n",
    "\n",
    "# We'll keep track of training and validation losses.\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "with tqdm_notebook(range(n_epoch)) as t:\n",
    "    for epoch in t:\n",
    "        net.train()  # Tell pytorch that we want to train the network.\n",
    "\n",
    "        # choose a random batch of samples\n",
    "        index_batch = np.random.choice(I, n_batch, replace=True)\n",
    "        x_batch = traj_train[index_batch, :]\n",
    "\n",
    "        # pass it through the network\n",
    "        z = net(x_batch)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = 0.5 * torch.mean(z**2) - torch.mean(net.log_jacobian(run_forward=False)) / 2.0\n",
    "\n",
    "        # take a gradient step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()                                               \n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():  # No gradients because we're validating.\n",
    "                z_val = net(traj_valid)\n",
    "                loss_val = 0.5 * torch.mean(z_val**2) - torch.mean(net.log_jacobian(run_forward=False)) / 2\n",
    "            losses.append(loss.item())\n",
    "            val_losses.append(loss_val.item())\n",
    "            scheduler.step(loss_val.item())\n",
    "            t.set_postfix(loss=loss.item(), val_loss=loss_val.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The training and validation losses have both plateaued. The validation loss is about the same as the training loss, indicating that we're not over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(losses)\n",
    "pp.plot(val_losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a set of structures from the network and plot them. These random latent samples map to structure that are similar to the input (see above). The one differnce is that there is a band of structures connecting the two points. I think this comes from the region where the two meta-stable regions butt against each other (see below). Since we're training by example, what matters is that examples map to high probability regions of the Gaussian. However, it doesn't say anything about the reverse—that is, where high-probabilty latent samples map to in structure-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_output = torch.normal(0, 1, (combined_traj.shape[0], 2), device=device)\n",
    "    x_output = net(z_output, rev=True).cpu()\n",
    "    pp.scatter(x_output[:, 0], x_output[:, 1], marker=\".\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how the training data maps into the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_left  = net(torch.as_tensor(traj_left,  dtype=torch.float32, device=device))\n",
    "z_right = net(torch.as_tensor(traj_right, dtype=torch.float32, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trans(rev, lim=6, n_grid=51):\n",
    "    net.eval()\n",
    "    n_dens = 5000\n",
    "    pp.figure(figsize=(9, 9))\n",
    "    \n",
    "    coarse_xs = torch.linspace(-lim, lim, n_grid, device=device)\n",
    "    coarse_ys = torch.linspace(-lim, lim, n_grid, device=device)\n",
    "    fine_xs = torch.linspace(-lim, lim, n_dens, device=device)\n",
    "    fine_ys = torch.linspace(-lim, lim, n_dens, device=device)\n",
    "    \n",
    "    for x in np.linspace(-lim, lim, n_grid):\n",
    "        points = torch.zeros((n_dens, 2), device=device)\n",
    "        points[:, 0] = x\n",
    "        points[:, 1] = fine_ys\n",
    "        with torch.no_grad():\n",
    "            trans = net(points, rev=rev)\n",
    "        trans = trans.cpu().detach().numpy()\n",
    "        pp.plot(trans[:, 0], trans[:, 1], color=\"grey\", linewidth=0.5)\n",
    "        \n",
    "    for y in coarse_ys:\n",
    "        points = torch.zeros((n_dens, 2), device=device)\n",
    "        points[:, 0] = fine_xs\n",
    "        points[:, 1] = y\n",
    "        with torch.no_grad():\n",
    "            trans = net(points, rev=rev)\n",
    "        trans = trans.cpu().detach().numpy()\n",
    "        pp.plot(trans[:, 0], trans[:, 1], color=\"grey\", linewidth=0.5)\n",
    "    pp.xlim(-3, 3)\n",
    "    pp.ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trans(rev=True, n_grid=31)\n",
    "pp.scatter(traj_left[:, 0], traj_left[:, 1], marker='.', alpha=0.2)\n",
    "pp.scatter(traj_right[:, 0], traj_right[:, 1], marker='.', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trans(rev=False, n_grid=51)\n",
    "z_left_cpu = z_left.cpu().detach().numpy()\n",
    "z_right_cpu = z_right.cpu().detach().numpy()\n",
    "pp.scatter(z_left_cpu[:, 0], z_left_cpu[:, 1], marker='.', alpha=0.1)\n",
    "pp.scatter(z_right_cpu[:, 0], z_right_cpu[:, 1], marker='.', alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_output = torch.normal(0, 1, (combined_traj.shape[0] * 50, 2), device=device)\n",
    "    x_output = net(z_output, rev=True)\n",
    "\n",
    "z_output = z_output.cpu().detach().numpy()\n",
    "x_output = x_output.cpu().detach().numpy()\n",
    "counts, edges = np.histogram(x_output[:, 0], range=(-3, 3), bins=50)\n",
    "edges = 0.5 * (edges[:-1] + edges[1:])\n",
    "keep_ind = np.where(counts > 0)\n",
    "edges = edges[keep_ind]\n",
    "counts = counts[keep_ind]\n",
    "free_energy = -np.log(counts)\n",
    "\n",
    "x_grid, energies = ener_model.get_dimer_energies()\n",
    "pp.plot(x_grid, energies);\n",
    "\n",
    "pp.scatter(edges, free_energy + (np.min(energies) - np.min(free_energy)), color=\"C1\")\n",
    "\n",
    "pp.ylim(-12, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetEnergy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, energy_model):\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.energy_model = energy_model\n",
    "        \n",
    "        n_batch = input.shape[0]\n",
    "        energies = torch.zeros((n_batch, 1))\n",
    "        \n",
    "        # get the coordinates on the cpu\n",
    "        input = input.cpu().detach().numpy()\n",
    "        for i in range(n_batch):\n",
    "            x = input[i, :]\n",
    "            energy = energy_model.energy(x)\n",
    "            energies[i, :] = energy\n",
    "        return energies.to(device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        energy_model = ctx.energy_model\n",
    "        \n",
    "        # move onto the cpu\n",
    "        input = input.cpu().detach().numpy()\n",
    "        grad_output = grad_output.cpu()\n",
    "        \n",
    "        n_batch = input.shape[0]\n",
    "        n_dim = input.shape[1]\n",
    "        grad_energy = torch.zeros((n_batch, n_dim))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            x = input[i, :]\n",
    "            forces = energy_model.forces(x).astype(\"float32\")\n",
    "            grad_energy[i, :] = grad_output[i] * torch.from_numpy(forces)\n",
    "        \n",
    "        return grad_energy.to(device=device), None\n",
    "\n",
    "get_energy = GetEnergy.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 1000\n",
    "n_batch = 1024\n",
    "\n",
    "# Build the optimizer.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "# We'll keep track of training and validation losses.\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Create a validation set\n",
    "z_val = torch.normal(0, 1, size=(n_batch, 2), device=device)\n",
    "\n",
    "with tqdm_notebook(range(n_epoch)) as t:\n",
    "    for epoch in t:\n",
    "        net.train()  # Tell pytorch that we want to train the network.\n",
    "        # generate a random batch of samples\n",
    "        index_batch = np.random.choice(I, n_batch, replace=True)\n",
    "        x_batch = traj_train[index_batch, :]\n",
    "\n",
    "        # pass it through the network\n",
    "        z = net(x_batch)\n",
    "\n",
    "        # compute the loss\n",
    "        loss1 = 0.5 * torch.mean(z**2) - torch.mean(net.log_jacobian(run_forward=False)) / 2\n",
    "\n",
    "        # generate anothr batch of samples\n",
    "        z_batch = torch.normal(0, 1, size=(n_batch, 2), device=device)\n",
    "\n",
    "        # run it back through the network\n",
    "        x = net(z_batch, rev=True)\n",
    "\n",
    "        # compute the loss\n",
    "        loss2 = torch.mean(get_energy(x, ener_model) - net.log_jacobian(run_forward=False, rev=True) / 2)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        # take a gradient step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():  # No gradients because we're validating.\n",
    "                x_val = net(z_val, rev=True)\n",
    "                loss_val = torch.mean(get_energy(x_val, ener_model) - net.log_jacobian(run_forward=False, rev=True) / 2)\n",
    "            losses.append(loss.item())\n",
    "            val_losses.append(loss_val.item())\n",
    "            scheduler.step(loss_val.item())\n",
    "            t.set_postfix(loss=loss.item(), val_loss=loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(losses)\n",
    "pp.plot(val_losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_output = torch.normal(0, 1, (combined_traj.shape[0], 2), device=device)\n",
    "    x_output = net(z_output, rev=True)\n",
    "z_output = z_output.cpu().detach().numpy()\n",
    "x_output = x_output.cpu().detach().numpy()\n",
    "ind_left = np.where(x_output[:, 0] < 0)\n",
    "ind_right = np.where(x_output[:, 0] >= 0)\n",
    "pp.figure(figsize=(7, 7))\n",
    "pp.scatter(x_output[ind_left, 0], x_output[ind_left, 1], marker=\".\", alpha=0.1)\n",
    "pp.scatter(x_output[ind_right, 0], x_output[ind_right, 1], marker=\".\", alpha=0.1)\n",
    "pp.xlim(-3, 3)\n",
    "pp.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trans(rev=True, n_grid=31)\n",
    "pp.scatter(x_output[ind_left, 0], x_output[ind_left, 1], marker='.', alpha=0.1)\n",
    "pp.scatter(x_output[ind_right, 0], x_output[ind_right, 1], marker='.', alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trans(rev=False, n_grid=51)\n",
    "pp.scatter(z_output[ind_left, 0], z_output[ind_left, 1], marker='.', alpha=0.1);\n",
    "pp.scatter(z_output[ind_right, 0], z_output[ind_right, 1], marker='.', alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_output = torch.normal(0, 1, (combined_traj.shape[0] * 50, 2), device=device)\n",
    "    x_output = net(z_output, rev=True)\n",
    "\n",
    "z_output = z_output.cpu().detach().numpy()\n",
    "x_output = x_output.cpu().detach().numpy()\n",
    "counts, edges = np.histogram(x_output[:, 0], range=(-3, 3), bins=50)\n",
    "edges = 0.5 * (edges[:-1] + edges[1:])\n",
    "keep_ind = np.where(counts > 0)\n",
    "edges = edges[keep_ind]\n",
    "counts = counts[keep_ind]\n",
    "free_energy = -np.log(counts)\n",
    "\n",
    "x_grid, energies = ener_model.get_dimer_energies()\n",
    "pp.plot(x_grid, energies);\n",
    "\n",
    "pp.scatter(edges, free_energy + (np.min(energies) - np.min(free_energy)), color=\"C1\")\n",
    "\n",
    "pp.ylim(-12, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
